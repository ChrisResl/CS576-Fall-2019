{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-8f3b19b382b45fcd",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## BMI\/CS 576 Fall 2019 - HW4\n",
                "The objectives of this homework are to practice\n",
                "\n",
                "* the Baum\u2013Welch algorithm\n",
                "* designing HMMs and evaluating their accuracy\n",
                "\n",
                "## HW policies\n",
                "Before starting this homework, please read over the [homework policies](https:\/\/canvas.wisc.edu\/courses\/167969\/pages\/hw-policies) for this course.  In particular, note that homeworks are to be completed *individually*.\n",
                "\n",
                "You are welcome to use any code from the weekly notebooks in your solutions to the HW."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-161ac9e9ef6c5517",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## PROBLEM 1: Baum\u2013Welch algorithm (60 points)\n",
                "\n",
                "For this problem you will implement the Baum\u2013Welch algorithm.  You will implement this algorithm as a set of methods defined for a subclass, `TrainableHMM`, of the HMM class that we have been developing in class.  The high-level master method for this algorithm, `estimate_parameters_baum_welch` is already implemented for you.  This method calls two other methods, corresponding to the Expectation (E) and Maximization (M) steps of the Baum\u2013Welch algorithm, which you are to implement.\n",
                "\n",
                "The base HMM class that we have implemented in the activities, along with a number of helper functions, are found in the `hmm` module included with this notebook.  You should add your solutions to key methods from your weekly notebooks to it and modify other aspects of it if you wish.  There are a few minor changes in how the HMM class works.  These should not affect how you implement the Baum\u2013Welch algorithm, but will be helpful in improving efficiency for the second problem of this HW.  The changes are:\n",
                "\n",
                "1. States may now be represented by arbitrary strings, rather than individual characters.  As a result, a state path (such as what is returned by the `most_probable_path` method) is now represented by a *list of strings*.\n",
                "2. The Viterbi, Forward, and Backward algorithms now take advantage of the sparsity (of non-zero transitions) of the HMM.  This results in efficiency gains, particular for HMMs with large numbers of states but relatively few transitions.  This is important for problem 2."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-adeead263575b62b",
                    "locked": false,
                    "schema_version": 1,
                    "solution": true
                }
            },
            "outputs": [],
            "source": [
                "import hmm\n",
                "import math\n",
                "import itertools\n",
                "from matplotlib import pyplot as plt\n",
                "\n",
                "class TrainableHMM(hmm.HiddenMarkovModel):\n",
                "\n",
                "    def estimate_parameters_baum_welch(self, \n",
                "                                       training_data, \n",
                "                                       pseudocount=0, \n",
                "                                       min_log_likelihood_diff=0.1,\n",
                "                                       verbose=False):\n",
                "        \"\"\"Estimates (and sets) the parameters of the model via the Baum\u2013Welch algorithm.\n",
                "        \n",
                "        The current parameters of the model will be used as initial parameters.\n",
                "        Args:\n",
                "            training_data: A list of strings of observed sequences.\n",
                "            pseudocount: a pseudocount to add to each expected count when \n",
                "                computing the parameter values.  The default is zero, which \n",
                "                corresponds to maximimum likelihood estimates without smoothing.\n",
                "                A value of one corresponds to Laplace smoothing.\n",
                "            min_log_likelihood_diff: the algorithm will terminate once the \n",
                "                difference in the log likelihood of the data from one iteration \n",
                "                to the next is less than this value.\n",
                "            verbose: if True, print progress information during the algorithm.\n",
                "        \"\"\"\n",
                "        # keep track of the log likelihood of the previous iteration\n",
                "        last_log_likelihood = float(\"-inf\")\n",
                "        \n",
                "        # repeat the E and M steps until convergence\n",
                "        for iteration in itertools.count():\n",
                "            # E-step\n",
                "            e_step_results = self.baum_welch_expectation_step(training_data)\n",
                "            log_likelihood = e_step_results[\"log_likelihood\"]\n",
                "\n",
                "            # optionally, print log likelihood at current iteration\n",
                "            if verbose: \n",
                "                print(\"Iteration %d:\" % iteration,\n",
                "                      \"log likelihood = %f\" % log_likelihood)\n",
                "            \n",
                "            # check if difference in log likelihood is sufficiently small to terminate            \n",
                "            if log_likelihood - last_log_likelihood < min_log_likelihood_diff:\n",
                "                if log_likelihood < last_log_likelihood:\n",
                "                    raise RuntimeError(\"Log likelihood is decreasing! \"\n",
                "                                       \"Some calculations must be incorrect.\")\n",
                "                return log_likelihood\n",
                "    \n",
                "            # M-step\n",
                "            self.baum_welch_maximization_step(e_step_results, pseudocount)\n",
                "            \n",
                "            last_log_likelihood = log_likelihood\n",
                "  \n",
                "    def baum_welch_expectation_step(self, training_data):\n",
                "        \"\"\"Runs the E-step of the Baum\u2013Welch algorithm.\n",
                "        \n",
                "        Args:\n",
                "            training_data: A list of strings of observed sequences.\n",
                "        Returns:\n",
                "            A dictionary with the results of the E-step, with the keys:\n",
                "                log_likelihood: the log probability of the training data given \n",
                "                    the current parameters\n",
                "                transition_count_matrix: a matrix of the same dimensions as \n",
                "                    transition_prob_matrix giving the expected counts of\n",
                "                    each transition\n",
                "                initial_counts: a list of the same length as initial_probs giving\n",
                "                    the expected counts of each state starting a hidden path \n",
                "                    (expected count of transition from begin state)\n",
                "                emission_count_matrix: a matrix of the same dimensions as \n",
                "                    emission_prob_matrix giving the expected counts of\n",
                "                    each emission\n",
                "        \"\"\"\n",
                "        # initialize matrices of counts\n",
                "        transition_count_matrix = hmm.matrix(len(self.states), len(self.states), 0)\n",
                "        initial_counts = [0] * len(self.states)\n",
                "        emission_count_matrix = hmm.matrix(len(self.states), len(self.chars), 0)\n",
                "        \n",
                "        # initialize log likelihood\n",
                "        log_likelihood = 0\n",
                "\n",
                "        # add contribution of each sequence to expected counts and log probability\n",
                "        for char_string in training_data:\n",
                "            ###\n",
                "            ### YOUR CODE HERE\n",
                "            ###\n",
                "            pass\n",
                "\n",
                "        # Return results as a dictionary\n",
                "        return {\"log_likelihood\": log_likelihood,\n",
                "                \"transition_count_matrix\": transition_count_matrix, \n",
                "                \"initial_counts\": initial_counts,\n",
                "                \"emission_count_matrix\": emission_count_matrix}\n",
                "\n",
                "    def baum_welch_maximization_step(self, e_step_result, pseudocount=0):\n",
                "        \"\"\"Runs the M-step of the Baum\u2013Welch algorithm, updating the parameters of the model.\n",
                "        \n",
                "        Args:\n",
                "            e_step_result: A dictionary of E-step results\n",
                "                           (see documentation for baum_welch_expectation_step)\n",
                "            pseudocount: a pseudocount to add to each expected count when \n",
                "                computing the parameter values.  The default is zero, which \n",
                "                corresponds to maximimum likelihood estimates without smoothing.\n",
                "                A value of one corresponds to Laplace smoothing.\n",
                "        \"\"\"\n",
                "        ###\n",
                "        ### YOUR CODE HERE\n",
                "        ###\n",
                "        pass"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-47da4ef56b0dd621",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "## PROBLEM 2: An exon-intron finding HMM (40 points)\n",
                "\n",
                "Recall that genes in the genomes of eukaryotic species have substructures called *exons* and *introns*, which are involved in a process called *splicing*.  The splicing process occurs after the initial transcription of the gene from DNA to RNA.  The initial RNA transcript is called a pre-mRNA and contains alternating exon and intron intervals.  A pre-mRNA always starts and ends with an exon and can have any number of introns (including zero, which would be a single-exon pre-mRNA).  During the splicing process, intron intervals are removed and consecutive exon intervals are linked together to form a mature mRNA.  An example of this process is illustrated below:\n",
                "\n",
                "![intron_exon](intron_exon.png)\n",
                "\n",
                "For this problem you are to devise and implement an HMM that models pre-mRNA sequences and their exon-intron structures.  You will then train your HMM on a set of pre-mRNA sequences from the fruit fly species *Drosophila melanogaster* using the Baum\u2013Welch algorithm.  With your trained HMM, you will then try to predict the locations of the exons and introns in the same set of pre-mRNA sequences."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-cb3b80351e51464e",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "**(A)** Draw the state transition diagram (the graph with states as nodes and transitions as edges), for the HMM you devise.  Your HMM should, at a bare minimum, have the following features:\n",
                "1. One or more states that model *exonic* positions.\n",
                "2. One or more states that model *intronic* positions.\n",
                "3. States that enforce that the first two bases of an intron are *always* `GU`\n",
                "4. States that enforce that the last two bases of an intron are *always* `AG`\n",
                "\n",
                "You are encouraged to experiment with more complex HMMs to see if you can improve the predictive performance of the model.  For example, you might consider the following features:\n",
                "\n",
                "1. States modeling the last two bases of an exon and the first six bases of an intron (which make up the \"donor\" splice site, the signal indicating the start of an intron).  See the notebook from day 10.\n",
                "2. States modeling the last six bases of an intron and the first base of an exon (which make up the \"acceptor\" splice site, the signal indicating the end of an intron).\n",
                "3. Higher-order models of the bases within exons and or introns.  For example, you can model the probability at which each base follows each other base within an exon (instead of each base within an exon being modeled independently).\n",
                "\n",
                "**(B)** Implement your HMM as an instance of the `TrainableHMM` class and train it (via Baum\u2013Welch) on the fly pre-mRNA sequences provided below.  Print out the estimated parameters of your model.  What statistical properties of exon and intron sequences is your model learning from the data?\n",
                "\n",
                "**(C)** With your trained model predict the locations of exons and introns within the same set of fly pre-mRNA sequences.  Compute some accuracy measures of your predictions with respect to the true exon-intron annotations provided.  For example, you could compute\n",
                "1. The base-level accuracy of your predictions: what fraction of the bases within the pre-mRNA sequences are corrected predicted as either exonic or intronic?\n",
                "2. The feature-level recall of your predictions: what fraction of the true exon (or intron) features are predicted correctly (e.g., for an exon to be predicted correctly, its start and end positions must both be predicted correctly).\n",
                "\n",
                "**(D)** With your trained model, and one (or more) of the provided pre-mRNA sequences, compute the posterior probability at each position that the position is exonic.  Make a plot of these posterior probabilities (i.e., a plot of the posterior probability of being in an exon vs. position in the sequence).  I recommend using the pyplot function [`fill_between`](https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.fill_between.html#matplotlib.pyplot.fill_between) for such a plot."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-e65d0556bd77ed3e",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### Fly RNA transcript data\n",
                "Below are the fly data that you are to use for this problem.  `fly_pre_mrnas` is a list of RNA strings, with each element of the list being the pre-mRNA for a different gene (gene names can be accessed in the corresponding `fly_gene_names` list).  `fly_pre_mrna_annotations` is a corresponding list of the true exon-intron structure of each pre-mRNA.  The structure is specified as a string of the same length as the pre-mRNA consisting of the characters `E` and `I`, indicating which positions of the pre-mRNA are exonic and intronic, respectively."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-42eba3be10590811",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "import fasta\n",
                "fly_pre_mrna_records = fasta.read_sequences_from_fasta_file(\"fly_transcripts.fasta\")\n",
                "fly_pre_mrna_annotation_records = fasta.read_sequences_from_fasta_file(\"fly_transcripts_exon_intron_annotation.fasta\")\n",
                "\n",
                "fly_gene_names, fly_pre_mrnas = zip(*fly_pre_mrna_records)\n",
                "fly_pre_mrna_annotations = tuple(zip(*fly_pre_mrna_annotation_records))[1]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Your solutions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "###\n",
                "### YOUR CODE HERE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Models for testing"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Coin flipping model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-4dc0f408fdb6171e",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# A coin flipping model like the casino model below, but with two\n",
                "# potentially biased coin states that emit either heads or tails\n",
                "coin_states = \"12\" # 1 = coin 1, 2 = coin 2\n",
                "coin_chars = \"HT\"  # heads or tails\n",
                "coin_initial_probs = [0.5, 0.5]\n",
                "coin_transition_prob_matrix = [\n",
                "    [0.8, 0.2],\n",
                "    [0.3, 0.7]\n",
                "]\n",
                "coin_emission_prob_matrix = [\n",
                "    [0.9, 0.1],\n",
                "    [0.4, 0.6]\n",
                "]\n",
                "coin_hmm = TrainableHMM(coin_states,\n",
                "                        coin_chars,\n",
                "                        coin_transition_prob_matrix,\n",
                "                        coin_initial_probs,\n",
                "                        coin_emission_prob_matrix)\n",
                "\n",
                "# A micro example with this model\n",
                "coin_micro_dataset = [\"T\"]\n",
                "\n",
                "# A tiny example with this model\n",
                "coin_tiny_dataset = [\"TH\"]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-31967adb20762453",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### Coin flipping model - micro tests (30 POINTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_micro_expected_emission_counts",
                    "locked": true,
                    "points": 6,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin micro expected emission_counts\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_micro_dataset)\n",
                "assert hmm.round_matrix(e_step_results[\"emission_count_matrix\"], 3) == [[0, 0.143], [0, 0.857]]\n",
                "print(\"SUCCESS: coin micro expected emission counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_micro_expected_transition_counts",
                    "locked": true,
                    "points": 6,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin micro expected transition_counts\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_micro_dataset)\n",
                "assert hmm.round_matrix(e_step_results[\"transition_count_matrix\"], 3) == [[0, 0], [0, 0]]\n",
                "print(\"SUCCESS: coin micro expected transition counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_micro_expected_initial_counts",
                    "locked": true,
                    "points": 6,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin micro expected initial counts\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_micro_dataset)\n",
                "assert hmm.round_vector(e_step_results[\"initial_counts\"], 3) == [0.143, 0.857]\n",
                "print(\"SUCCESS: coin micro expected initial counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_micro_log_likelihood",
                    "locked": true,
                    "points": 6,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin micro log likelihood\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_micro_dataset)\n",
                "assert round(e_step_results[\"log_likelihood\"], 3) == -1.05\n",
                "print(\"SUCCESS: coin micro log likelihood test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_micro_M-step",
                    "locked": true,
                    "points": 6,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin micro M-step\n",
                "e_step_results = {\n",
                "    \"transition_count_matrix\": [[0, 0], \n",
                "                                [0, 0]],\n",
                "    \"initial_counts\": [0.14285714285714288, 0.8571428571428572],\n",
                "    \"emission_count_matrix\": [[0, 0.14285714285714288], \n",
                "                              [0, 0.8571428571428572]],\n",
                "    \"log_likelihood\": -1.0498221244986776}\n",
                "\n",
                "pseudocount = 1\n",
                "coin_hmm.baum_welch_maximization_step(e_step_results, pseudocount)\n",
                "assert hmm.round_matrix(coin_hmm.transition_prob_matrix, 3) == [[0.5, 0.5], [0.5, 0.5]]\n",
                "assert hmm.round_vector(coin_hmm.initial_probs, 3) == [0.381, 0.619]\n",
                "assert hmm.round_matrix(coin_hmm.emission_prob_matrix, 3) == [[0.467, 0.533], [0.35, 0.65]]\n",
                "print(\"SUCCESS: coin micro M-step test passed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-69624639e6aedd2f",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### Coin flipping model - tiny tests (10 POINTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_tiny_expected_emission_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin tiny expected emission_counts\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_tiny_dataset)\n",
                "assert hmm.round_matrix(e_step_results[\"emission_count_matrix\"], 3) == [[0.571, 0.195], [0.429, 0.805]]\n",
                "print(\"SUCCESS: coin tiny expected emission counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_tiny_expected_transition_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin tiny expected transition_counts\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_tiny_dataset)\n",
                "assert hmm.round_matrix(e_step_results[\"transition_count_matrix\"], 3) == [[0.176, 0.02], [0.395, 0.41]]\n",
                "print(\"SUCCESS: coin tiny expected transition counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_tiny_expected_initial_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin tiny expected initial counts\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_tiny_dataset)\n",
                "assert hmm.round_vector(e_step_results[\"initial_counts\"], 3) == [0.195, 0.805]\n",
                "print(\"SUCCESS: coin tiny expected initial counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_tiny_log_likelihood",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin tiny log likelihood\n",
                "coin_hmm.set_parameters(coin_transition_prob_matrix, coin_initial_probs, coin_emission_prob_matrix)\n",
                "e_step_results = coin_hmm.baum_welch_expectation_step(coin_tiny_dataset)\n",
                "assert round(e_step_results[\"log_likelihood\"], 3) == -1.585\n",
                "print(\"SUCCESS: coin tiny log likelihood test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "coin_tiny_M-step",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for coin tiny M-step\n",
                "e_step_results = {\n",
                "    \"transition_count_matrix\":[[0.175609756097561, 0.019512195121951226], \n",
                "                               [0.3951219512195122, 0.4097560975609756]],\n",
                "    \"initial_counts\":        [0.19512195121951223, 0.8048780487804876],\n",
                "    \"emission_count_matrix\": [[0.5707317073170731, 0.19512195121951223], \n",
                "                              [0.4292682926829269, 0.8048780487804876]],\n",
                "    \"log_likelihood\": -1.5847452998437288}\n",
                "pseudocount = 0\n",
                "coin_hmm.baum_welch_maximization_step(e_step_results, pseudocount)\n",
                "assert hmm.round_matrix(coin_hmm.transition_prob_matrix, 3) == [[0.9, 0.1], [0.491, 0.509]]\n",
                "assert hmm.round_vector(coin_hmm.initial_probs, 3) == [0.195, 0.805]\n",
                "assert hmm.round_matrix(coin_hmm.emission_prob_matrix, 3) == [[0.745, 0.255], [0.348, 0.652]]\n",
                "print(\"SUCCESS: coin tiny M-step test passed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-bbce6bef6c09c0df",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### Occasionally dishonest casino tests (10 POINTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-1f8c6c33e03531c9",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# The occasionally dishonest casino example described in the lecture and textbook\n",
                "casino_states = \"FL\"     # F = fair die, L = loaded die\n",
                "casino_chars = \"123456\"  # the six sides of the die\n",
                "casino_initial_probs = [0.5, 0.5]\n",
                "casino_transition_prob_matrix = [\n",
                "    [0.95, 0.05],\n",
                "    [0.10, 0.90]\n",
                "]\n",
                "casino_emission_prob_matrix = [\n",
                "    [ 1\/6,  1\/6,  1\/6,  1\/6,  1\/6, 1\/6],\n",
                "    [1\/10, 1\/10, 1\/10, 1\/10, 1\/10, 1\/2]\n",
                "]\n",
                "casino_hmm = TrainableHMM(casino_states, \n",
                "                          casino_chars, \n",
                "                          casino_transition_prob_matrix, \n",
                "                          casino_initial_probs,\n",
                "                          casino_emission_prob_matrix)\n",
                "\n",
                "casino_sequences = [\"165\", \"63\"]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "casino_expected_emission_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for casino expected emission counts\n",
                "casino_hmm.set_parameters(casino_transition_prob_matrix, casino_initial_probs, casino_emission_prob_matrix)\n",
                "e_step_results = casino_hmm.baum_welch_expectation_step(casino_sequences)\n",
                "assert hmm.round_matrix(e_step_results[\"emission_count_matrix\"], 3) == \\\n",
                "    [[0.484, 0, 0.431, 0, 0.535, 0.804], \n",
                "     [0.516, 0, 0.569, 0, 0.465, 1.196]]\n",
                "print(\"SUCCESS: casino expected emission counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "casino_expected_transition_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for casino expected transition counts\n",
                "casino_hmm.set_parameters(casino_transition_prob_matrix, casino_initial_probs, casino_emission_prob_matrix)\n",
                "e_step_results = casino_hmm.baum_welch_expectation_step(casino_sequences)\n",
                "log_likelihood, transition_count_matrix, initial_counts, emission_count_matrix = e_step_results\n",
                "assert hmm.round_matrix(e_step_results[\"transition_count_matrix\"], 3) == \\\n",
                "    [[1.218, 0.07], [0.215, 1.497]]\n",
                "print(\"SUCCESS: casino expected transition counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "casino_expected_initial_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for casino expected initial counts\n",
                "casino_hmm.set_parameters(casino_transition_prob_matrix, casino_initial_probs, casino_emission_prob_matrix)\n",
                "e_step_results = casino_hmm.baum_welch_expectation_step(casino_sequences)\n",
                "assert hmm.round_vector(e_step_results[\"initial_counts\"], 3) == [0.822, 1.178]\n",
                "print(\"SUCCESS: casino expected initial counts test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "casino_log_likelihood",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": false,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for casino log likelihood\n",
                "casino_hmm.set_parameters(casino_transition_prob_matrix, casino_initial_probs, casino_emission_prob_matrix)\n",
                "e_step_results = casino_hmm.baum_welch_expectation_step(casino_sequences)\n",
                "assert round(e_step_results[\"log_likelihood\"], 3) == -8.528\n",
                "print(\"SUCCESS: casino log likelihood test passed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "casino_M-step",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for casino M-step\n",
                "e_step_results = {\n",
                "    \"transition_count_matrix\": [[1.2182391796657972, 0.06984732452392478],\n",
                "                                [0.21453844829210775, 1.497375047518171]],\n",
                "    \"initial_counts\": [0.8217716073650729, 1.178228392634928],\n",
                "    \"emission_count_matrix\": [\n",
                "        [0.48384057288231425, 0, 0.4310344827586207, 0, 0.5354282483746345, 0.8042459313074077],\n",
                "        [0.5161594271176861, 0, 0.5689655172413794, 0, 0.46457175162536557, 1.1957540686925925]]\n",
                "}\n",
                "\n",
                "pseudocount = 0\n",
                "casino_hmm.baum_welch_maximization_step(e_step_results, pseudocount)\n",
                "assert hmm.round_matrix(casino_hmm.transition_prob_matrix, 3) == [[0.946, 0.054], [0.125, 0.875]]\n",
                "assert hmm.round_vector(casino_hmm.initial_probs, 3) == [0.411, 0.589]\n",
                "assert hmm.round_matrix(casino_hmm.emission_prob_matrix, 3) == [[0.215, 0.0, 0.191, 0.0, 0.237, 0.357],\n",
                "                                                                [0.188, 0.0, 0.207, 0.0, 0.169, 0.436]]\n",
                "print(\"SUCCESS: casino M-step test passed\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-c076c6a8b97b5676",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "source": [
                "### CpG model tests (10 POINTS)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": false,
                    "grade_id": "cell-ec7fe235d646fed2",
                    "locked": true,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": true,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# The CpG HMM example described in the lecture and textbook\n",
                "cpg_states = [\"A+\", \"C+\", \"G+\", \"T+\", # CpG states\n",
                "              \"A-\", \"C-\", \"G-\", \"T-\"] # Null states\n",
                "cpg_chars = \"ACGT\"\n",
                "\n",
                "cpg_initial_probs = [1 \/ len(cpg_states)] * len(cpg_states)\n",
                "\n",
                "cpg_transition_prob_matrix = hmm.matrix(len(cpg_states), len(cpg_states))\n",
                "for i in range(len(cpg_states)):\n",
                "    for j in range(len(cpg_states)):\n",
                "        same_class = cpg_states[i][-1] == cpg_states[j][-1]\n",
                "        cpg_transition_prob_matrix[i][j] = 0.2 if same_class else 0.05\n",
                "# modify transition probs from C- to model having fewer CpG in null\n",
                "cpg_transition_prob_matrix[cpg_states.index(\"C-\")][cpg_states.index(\"A-\")] = 0.25\n",
                "cpg_transition_prob_matrix[cpg_states.index(\"C-\")][cpg_states.index(\"C-\")] = 0.25\n",
                "cpg_transition_prob_matrix[cpg_states.index(\"C-\")][cpg_states.index(\"G-\")] = 0.05\n",
                "cpg_transition_prob_matrix[cpg_states.index(\"C-\")][cpg_states.index(\"T-\")] = 0.25\n",
                "\n",
                "cpg_emission_prob_matrix = hmm.matrix(len(cpg_states), len(cpg_chars))\n",
                "for i in range(len(cpg_states)):\n",
                "    for j in range(len(cpg_chars)):\n",
                "        char_matches_state = cpg_states[i][0] == cpg_chars[j]\n",
                "        cpg_emission_prob_matrix[i][j] = 1 if char_matches_state else 0\n",
                "\n",
                "cpg_hmm = TrainableHMM(cpg_states, \n",
                "                       cpg_chars, \n",
                "                       cpg_transition_prob_matrix, \n",
                "                       cpg_initial_probs,\n",
                "                       cpg_emission_prob_matrix)\n",
                "\n",
                "cpg_dataset = [\n",
                "    'CCTTGATTAAATGAGGAAAGTGCTATCATGTTTGTCCCTACAATCTTGCTCGCAGCCGCATGGTATCCCCAATTCCGGCT',\n",
                "    'CCAGGAAGTAGACTCTTCTGCGGCGAGAGGGGGGCCGGAGCATGGCTCAGGTCACATGAGCGATGTCAACTAGTGCGAGT',\n",
                "    'TTATTGGCCTACGAAAATCTACAAGAGGTTTTGACAGTTGTGAGAATCGGGCGGTACCACTCATCGATAGGGCTGAGCAT',\n",
                "    'GGGGTCTATGGCGGCGAGTCCGTCTAATTACTGACAGGGCAGGTCTTATGGCCGACGGTAAACCATGCACGCACCGGGGT',\n",
                "    'ATTGATGCCCGGATGAGGGCGCGACCCAAGACATCGACATCAAGTGGAACAGCGAGTCTCTTGTCATCAGGGCGGCCCCC',\n",
                "    'AACTACTCCTATGTTATGGTCTCGGAGGGAATTCCACCAATTAGCTGGGAAGCAGCCCCATTTTGCTTCAACTAATTCGA',\n",
                "    'CTAGCTCCACTGACGGGTGTATTCAGCCGTACTAATATCGGGACTACCAGCCTGTCTTGACCGCAGCTGGCCGACCTAAC',\n",
                "    'AGTATTGAACGTTTAACTCTCAGGACAGGGCGTCTGCATTCTGTTCACATACTGTAAATGAGCAATATCCTTTTTGAGTC',\n",
                "    'TCGTTTGTAGGTGTGAGACTTCCTAAGGTACAGGATAGCTTATTTGGGAGAATACCTTCCACCTCTGCATCCAGGCTATC',\n",
                "    'TCCCTGTGTGTTAGCGAAAATTATGGTTACAAGCATCCAATACAACTTTCATCTCCAAGCTAGTCGTAAAAGCTGATAAA']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cpg_expected_emission_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for cpg expected emission counts\n",
                "cpg_hmm.set_parameters(cpg_transition_prob_matrix, cpg_initial_probs, cpg_emission_prob_matrix)\n",
                "e_step_results = cpg_hmm.baum_welch_expectation_step(cpg_dataset)\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cpg_expected_transition_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for cpg expected transition counts\n",
                "cpg_hmm.set_parameters(cpg_transition_prob_matrix, cpg_initial_probs, cpg_emission_prob_matrix)\n",
                "e_step_results = cpg_hmm.baum_welch_expectation_step(cpg_dataset)\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cpg_expected_initial_counts",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for cpg expected initial counts\n",
                "cpg_hmm.set_parameters(cpg_transition_prob_matrix, cpg_initial_probs, cpg_emission_prob_matrix)\n",
                "e_step_results = cpg_hmm.baum_welch_expectation_step(cpg_dataset)\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cpg_log_likelihood",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "scrolled": false,
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for cpg log likelihood\n",
                "cpg_hmm.set_parameters(cpg_transition_prob_matrix, cpg_initial_probs, cpg_emission_prob_matrix)\n",
                "e_step_results = cpg_hmm.baum_welch_expectation_step(cpg_dataset)\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "cpg_M-step",
                    "locked": true,
                    "points": 2,
                    "schema_version": 1,
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "# tests for cpg M-step\n",
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        }
    ],
    "metadata": {
        "celltoolbar": "Create Assignment",
        "kernelspec": {
            "display_name": "Python 3 [3.6]",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}